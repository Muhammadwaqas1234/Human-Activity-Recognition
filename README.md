# ğŸ“± Human Activity Recognition (HAR) using LSTM

A **real-time Human Activity Recognition system** that uses **mobile phone motion sensors** (accelerometer + gyroscope) and a **deep learning LSTM model** to classify human activities such as walking, sitting, standing, and lying.

The system includes:

* A **TensorFlow/Keras LSTM model**
* A **FastAPI backend**
* A **mobile-friendly web frontend**
* Real-time sensor data capture from Android devices

---

## ğŸš€ Features

* ğŸ“Š LSTM-based sequence modeling (trained on UCI HAR Dataset)
* ğŸ“± Live mobile sensor data (DeviceMotion API)
* ğŸŒ Web-based UI accessible from phone browser
* âš¡ Real-time predictions with confidence score
* ğŸ” Automatic padding to 128 timesteps
* ğŸ§  Activities detected:

  * Walking
  * Walking Upstairs
  * Walking Downstairs
  * Sitting
  * Standing
  * Lying

---

## ğŸ§  Model Details

* **Architecture:** LSTM â†’ Dense â†’ Softmax

* **Input Shape:** `(128 timesteps, 9 features)`

* **Features Used:**

  * Body Acceleration (X, Y, Z)
  * Gyroscope (X, Y, Z)
  * Total Acceleration (X, Y, Z)

* **Framework:** TensorFlow / Keras

* **Saved Model:** `har_lstm_model.h5`

---

## ğŸ—‚ï¸ Project Structure

```
HAR_App/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py          # FastAPI server
â”‚   â”œâ”€â”€ model.py         # LSTM model loader & inference
â”‚   â”œâ”€â”€ schemas.py       # Request validation
â”‚   â””â”€â”€ har_lstm_model.h5
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html       # UI
â”‚   â”œâ”€â”€ style.css        # Styling
â”‚   â””â”€â”€ script.js        # Mobile motion logic
â”‚
â”œâ”€â”€ venv/                # Virtual environment
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd HAR_App
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install fastapi uvicorn tensorflow numpy pydantic
```

---

### 4ï¸âƒ£ Run Backend Server

```bash
uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload
```

---

## ğŸ“² How to Use (Mobile)

1. Connect **mobile & laptop to same Wi-Fi**
2. Open browser on phone
3. Visit:

```
http://<laptop-ip>:8000
```

Example:

```
http://192.168.18.93:8000
```

4. Allow **Motion Sensor Permission**
5. Move your phone (walk, shake, sit)
6. See live activity & confidence ğŸ¯

---

## ğŸŒ API Endpoint

### `POST /api/predict`

**Request Body**

```json
{
  "data": [
    [ax, ay, az, gx, gy, gz, tax, tay, taz],
    ...
  ]
}
```

**Response**

```json
{
  "activity": "Walking",
  "confidence": 0.92
}
```

---

## ğŸ”’ Notes & Limitations

* HTTPS is required on some mobile browsers for motion sensors
* Sensor values differ from UCI HAR dataset â†’ accuracy may vary
* Model trained on **waist-mounted sensors**, phone position matters

---

## ğŸ› ï¸ Future Improvements

* ğŸ” Sensor normalization (mean/std)
* ğŸ“¦ Convert model to TensorFlow Lite
* ğŸ“² Android APK / PWA
* ğŸ“ˆ Temporal smoothing of predictions
* ğŸ§ª Fine-tuning on real mobile sensor data

---

## ğŸ‘¨â€ğŸ’» Author

**Muhammad Waqas**
AI | Deep Learning | Computer Vision
ğŸ“ Pakistan

---

## ğŸ“œ License

This project is for **educational & research purposes**.


